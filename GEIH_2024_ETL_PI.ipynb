{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60787cfa",
   "metadata": {},
   "source": [
    "# Proyecto GEIH 2024 - ETL en PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d00afc2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1749523038563_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-28-10.ec2.internal:20888/proxy/application_1749523038563_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-20-52.ec2.internal:8042/node/containerlogs/container_1749523038563_0001_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f7d109472b4efcb07eeaeb4aaaf40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4e710a83e3426f834412b4882ea992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"GEIH_ETL_2024\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea2f392",
   "metadata": {},
   "source": [
    "## Funciones para unión segura y procesamiento mensual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d82146ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a4f685d2994407689c30b075f799625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def safe_join(df1, df2, claves, prefix):\n",
    "    renamed = df2.select([col(c).alias(f\"{prefix.lower()}_{c.lower()}\") if c not in claves else col(c) for c in df2.columns])\n",
    "    return df1.join(renamed, on=claves, how=\"left\")\n",
    "\n",
    "def procesar_mes(mes, modulos):\n",
    "    ruta_mes = f\"s3://geih-datalake-2024/raw/{mes}/\"\n",
    "    datos = {}\n",
    "    for modulo, claves in modulos.items():\n",
    "        try:\n",
    "            df = spark.read.option(\"header\", True).option(\"sep\", \";\").option(\"encoding\", \"latin1\").csv(ruta_mes + f\"{modulo}.CSV\")\n",
    "            datos[modulo] = df\n",
    "            print(f\"[{mes}] Módulo {modulo} cargado.\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{mes}] Error módulo {modulo}: {e}\")\n",
    "\n",
    "    df_base = datos[\"10\"]\n",
    "    for cod in [\"50\", \"60\", \"70\", \"80\", \"90\", \"94\"]:\n",
    "        if cod in datos:\n",
    "            df_base = safe_join(df_base, datos[cod], [\"DIRECTORIO\", \"SECUENCIA_P\", \"ORDEN\"], cod)\n",
    "    df_base = safe_join(df_base, datos[\"01\"], [\"DIRECTORIO\", \"SECUENCIA_P\"], \"01\")\n",
    "    df_base = df_base.select([col(c).alias(c.lower().strip().replace(\" \", \"_\")) for c in df_base.columns])\n",
    "    df_base = df_base.dropna(subset=[\"directorio\", \"secuencia_p\", \"orden\"])\n",
    "    ruta_salida = f\"s3://geih-datalake-2024/trusted/{mes}/\"\n",
    "    df_base.write.mode(\"overwrite\").parquet(ruta_salida)\n",
    "    print(f\"Datos de {mes} guardados en zona trusted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef17d23",
   "metadata": {},
   "source": [
    "## Ejecución del procesamiento para todos los meses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8075f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693118f332d44ac5b98ee2a536a1e8c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[enero] M?dulo 01 cargado.\n",
      "[enero] M?dulo 10 cargado.\n",
      "[enero] M?dulo 50 cargado.\n",
      "[enero] M?dulo 60 cargado.\n",
      "[enero] M?dulo 70 cargado.\n",
      "[enero] M?dulo 80 cargado.\n",
      "[enero] M?dulo 90 cargado.\n",
      "[enero] M?dulo 94 cargado.\n",
      "Datos de enero guardados en zona trusted.\n",
      "[febrero] M?dulo 01 cargado.\n",
      "[febrero] M?dulo 10 cargado.\n",
      "[febrero] M?dulo 50 cargado.\n",
      "[febrero] M?dulo 60 cargado.\n",
      "[febrero] M?dulo 70 cargado.\n",
      "[febrero] M?dulo 80 cargado.\n",
      "[febrero] M?dulo 90 cargado.\n",
      "[febrero] M?dulo 94 cargado.\n",
      "Datos de febrero guardados en zona trusted.\n",
      "[marzo] M?dulo 01 cargado.\n",
      "[marzo] M?dulo 10 cargado.\n",
      "[marzo] M?dulo 50 cargado.\n",
      "[marzo] M?dulo 60 cargado.\n",
      "[marzo] M?dulo 70 cargado.\n",
      "[marzo] M?dulo 80 cargado.\n",
      "[marzo] M?dulo 90 cargado.\n",
      "[marzo] M?dulo 94 cargado.\n",
      "Datos de marzo guardados en zona trusted.\n",
      "[abril] M?dulo 01 cargado.\n",
      "[abril] M?dulo 10 cargado.\n",
      "[abril] M?dulo 50 cargado.\n",
      "[abril] M?dulo 60 cargado.\n",
      "[abril] M?dulo 70 cargado.\n",
      "[abril] M?dulo 80 cargado.\n",
      "[abril] M?dulo 90 cargado.\n",
      "[abril] M?dulo 94 cargado.\n",
      "Datos de abril guardados en zona trusted.\n",
      "[mayo] M?dulo 01 cargado.\n",
      "[mayo] M?dulo 10 cargado.\n",
      "[mayo] M?dulo 50 cargado.\n",
      "[mayo] M?dulo 60 cargado.\n",
      "[mayo] M?dulo 70 cargado.\n",
      "[mayo] M?dulo 80 cargado.\n",
      "[mayo] M?dulo 90 cargado.\n",
      "[mayo] M?dulo 94 cargado.\n",
      "Datos de mayo guardados en zona trusted.\n",
      "[junio] M?dulo 01 cargado.\n",
      "[junio] M?dulo 10 cargado.\n",
      "[junio] M?dulo 50 cargado.\n",
      "[junio] M?dulo 60 cargado.\n",
      "[junio] M?dulo 70 cargado.\n",
      "[junio] M?dulo 80 cargado.\n",
      "[junio] M?dulo 90 cargado.\n",
      "[junio] M?dulo 94 cargado.\n",
      "Datos de junio guardados en zona trusted.\n",
      "[julio] M?dulo 01 cargado.\n",
      "[julio] M?dulo 10 cargado.\n",
      "[julio] M?dulo 50 cargado.\n",
      "[julio] M?dulo 60 cargado.\n",
      "[julio] M?dulo 70 cargado.\n",
      "[julio] M?dulo 80 cargado.\n",
      "[julio] M?dulo 90 cargado.\n",
      "[julio] M?dulo 94 cargado.\n",
      "Datos de julio guardados en zona trusted.\n",
      "[agosto] M?dulo 01 cargado.\n",
      "[agosto] M?dulo 10 cargado.\n",
      "[agosto] M?dulo 50 cargado.\n",
      "[agosto] M?dulo 60 cargado.\n",
      "[agosto] M?dulo 70 cargado.\n",
      "[agosto] M?dulo 80 cargado.\n",
      "[agosto] M?dulo 90 cargado.\n",
      "[agosto] M?dulo 94 cargado.\n",
      "Datos de agosto guardados en zona trusted.\n",
      "[septiembre] M?dulo 01 cargado.\n",
      "[septiembre] M?dulo 10 cargado.\n",
      "[septiembre] M?dulo 50 cargado.\n",
      "[septiembre] M?dulo 60 cargado.\n",
      "[septiembre] M?dulo 70 cargado.\n",
      "[septiembre] M?dulo 80 cargado.\n",
      "[septiembre] M?dulo 90 cargado.\n",
      "[septiembre] M?dulo 94 cargado.\n",
      "Datos de septiembre guardados en zona trusted.\n",
      "[octubre] M?dulo 01 cargado.\n",
      "[octubre] M?dulo 10 cargado.\n",
      "[octubre] M?dulo 50 cargado.\n",
      "[octubre] M?dulo 60 cargado.\n",
      "[octubre] M?dulo 70 cargado.\n",
      "[octubre] M?dulo 80 cargado.\n",
      "[octubre] M?dulo 90 cargado.\n",
      "[octubre] M?dulo 94 cargado.\n",
      "Datos de octubre guardados en zona trusted.\n",
      "[noviembre] M?dulo 01 cargado.\n",
      "[noviembre] M?dulo 10 cargado.\n",
      "[noviembre] M?dulo 50 cargado.\n",
      "[noviembre] M?dulo 60 cargado.\n",
      "[noviembre] M?dulo 70 cargado.\n",
      "[noviembre] M?dulo 80 cargado.\n",
      "[noviembre] M?dulo 90 cargado.\n",
      "[noviembre] M?dulo 94 cargado.\n",
      "Datos de noviembre guardados en zona trusted.\n",
      "[diciembre] M?dulo 01 cargado.\n",
      "[diciembre] M?dulo 10 cargado.\n",
      "[diciembre] M?dulo 50 cargado.\n",
      "[diciembre] M?dulo 60 cargado.\n",
      "[diciembre] M?dulo 70 cargado.\n",
      "[diciembre] M?dulo 80 cargado.\n",
      "[diciembre] M?dulo 90 cargado.\n",
      "[diciembre] M?dulo 94 cargado.\n",
      "Datos de diciembre guardados en zona trusted."
     ]
    }
   ],
   "source": [
    "modulos = {\n",
    "    \"01\": [\"DIRECTORIO\", \"SECUENCIA_P\"],\n",
    "    \"10\": [\"DIRECTORIO\", \"SECUENCIA_P\", \"ORDEN\"],\n",
    "    \"50\": [\"DIRECTORIO\", \"SECUENCIA_P\", \"ORDEN\"],\n",
    "    \"60\": [\"DIRECTORIO\", \"SECUENCIA_P\", \"ORDEN\"],\n",
    "    \"70\": [\"DIRECTORIO\", \"SECUENCIA_P\", \"ORDEN\"],\n",
    "    \"80\": [\"DIRECTORIO\", \"SECUENCIA_P\", \"ORDEN\"],\n",
    "    \"90\": [\"DIRECTORIO\", \"SECUENCIA_P\", \"ORDEN\"],\n",
    "    \"94\": [\"DIRECTORIO\", \"SECUENCIA_P\", \"ORDEN\"]\n",
    "}\n",
    "\n",
    "meses = [\"enero\", \"febrero\", \"marzo\", \"abril\", \"mayo\", \"junio\",\n",
    "         \"julio\", \"agosto\", \"septiembre\", \"octubre\", \"noviembre\", \"diciembre\"]\n",
    "\n",
    "for mes in meses:\n",
    "    procesar_mes(mes, modulos)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d935de",
   "metadata": {},
   "source": [
    "## Consolidación de todos los meses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4efec2a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9606ff370d52490a9659f211ff366bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o28400.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 206.0 failed 4 times, most recent failure: Lost task 0.3 in stage 206.0 (TID 369) (ip-172-31-20-52.ec2.internal executor 2): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to s3://geih-datalake-2024/trusted/geih2024_full.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:789)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:421)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://geih-datalake-2024/trusted/geih2024_full/part-00003-a060e0fb-4b0b-4caf-8c8b-0e12937b34f5-c000.snappy.parquet, range: 0-11928485, partition values: [empty row], isDataPresent: false, eTag: 81ed22a4f6ae0966644baae33ea92d27-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.$anonfun$next$2(AsyncFileDownloader.scala:130)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:447)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:359)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:239)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:404)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1575)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:411)\n",
      "\t... 15 more\n",
      "Caused by: java.io.FileNotFoundException: No such file or directory 's3://geih-datalake-2024/trusted/geih2024_full/part-00003-a060e0fb-4b0b-4caf-8c8b-0e12937b34f5-c000.snappy.parquet'\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:535)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:623)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:523)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:69)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2974)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2910)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2909)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2909)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1263)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1263)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1263)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3173)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3112)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3101)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1028)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2264)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:197)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:250)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:250)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$8(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:271)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:159)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:101)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:554)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:554)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:530)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to s3://geih-datalake-2024/trusted/geih2024_full.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:789)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:421)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://geih-datalake-2024/trusted/geih2024_full/part-00003-a060e0fb-4b0b-4caf-8c8b-0e12937b34f5-c000.snappy.parquet, range: 0-11928485, partition values: [empty row], isDataPresent: false, eTag: 81ed22a4f6ae0966644baae33ea92d27-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.$anonfun$next$2(AsyncFileDownloader.scala:130)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:447)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:359)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:239)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:404)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1575)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:411)\n",
      "\t... 15 more\n",
      "Caused by: java.io.FileNotFoundException: No such file or directory 's3://geih-datalake-2024/trusted/geih2024_full/part-00003-a060e0fb-4b0b-4caf-8c8b-0e12937b34f5-c000.snappy.parquet'\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:535)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:623)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:523)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:69)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1749523038563_0001/container_1749523038563_0001_01_000001/pyspark.zip/pyspark/sql/readwriter.py\", line 1656, in parquet\n",
      "    self._jwrite.parquet(path)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1749523038563_0001/container_1749523038563_0001_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1323, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1749523038563_0001/container_1749523038563_0001_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 169, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1749523038563_0001/container_1749523038563_0001_01_000001/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o28400.parquet.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 206.0 failed 4 times, most recent failure: Lost task 0.3 in stage 206.0 (TID 369) (ip-172-31-20-52.ec2.internal executor 2): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to s3://geih-datalake-2024/trusted/geih2024_full.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:789)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:421)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://geih-datalake-2024/trusted/geih2024_full/part-00003-a060e0fb-4b0b-4caf-8c8b-0e12937b34f5-c000.snappy.parquet, range: 0-11928485, partition values: [empty row], isDataPresent: false, eTag: 81ed22a4f6ae0966644baae33ea92d27-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.$anonfun$next$2(AsyncFileDownloader.scala:130)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:447)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:359)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:239)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:404)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1575)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:411)\n",
      "\t... 15 more\n",
      "Caused by: java.io.FileNotFoundException: No such file or directory 's3://geih-datalake-2024/trusted/geih2024_full/part-00003-a060e0fb-4b0b-4caf-8c8b-0e12937b34f5-c000.snappy.parquet'\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:535)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:623)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:523)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:69)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2974)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2910)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2909)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2909)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1263)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1263)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1263)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3173)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3112)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3101)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1028)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2264)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:197)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:250)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:250)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$8(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:271)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:159)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:69)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:101)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:554)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:107)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:554)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:530)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:97)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:82)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to s3://geih-datalake-2024/trusted/geih2024_full.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:789)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:421)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://geih-datalake-2024/trusted/geih2024_full/part-00003-a060e0fb-4b0b-4caf-8c8b-0e12937b34f5-c000.snappy.parquet, range: 0-11928485, partition values: [empty row], isDataPresent: false, eTag: 81ed22a4f6ae0966644baae33ea92d27-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.$anonfun$next$2(AsyncFileDownloader.scala:130)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:447)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:359)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:239)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:404)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1575)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:411)\n",
      "\t... 15 more\n",
      "Caused by: java.io.FileNotFoundException: No such file or directory 's3://geih-datalake-2024/trusted/geih2024_full/part-00003-a060e0fb-4b0b-4caf-8c8b-0e12937b34f5-c000.snappy.parquet'\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:535)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:623)\n",
      "\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:523)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:69)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_geih_2024 = spark.read.parquet(\"s3://geih-datalake-2024/trusted/*/\")\n",
    "df_geih_2024.write.mode(\"overwrite\").parquet(\"s3://geih-datalake-2024/trusted/geih2024_full/\")\n",
    "df_geih_2024.printSchema()\n",
    "\n",
    "print(\"Consolidado anual guardado en zona trusted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1852a2",
   "metadata": {},
   "source": [
    "## Eliminación de columnas con más del 40% de nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e52fd54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb7276d4663b462cae6005f0827c95c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o28377.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 204.0 failed 4 times, most recent failure: Lost task 0.3 in stage 204.0 (TID 359) (ip-172-31-20-52.ec2.internal executor 1): org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://geih-datalake-2024/trusted/abril/part-00000-b6d0950b-7bd3-4756-91a7-9736e3b02936-c000.snappy.parquet, range: 0-5953956, partition values: [empty row], isDataPresent: false, eTag: d038e68fdfc6f3ea49f6e394aa9659a6-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:139)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:447)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:359)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:239)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:731)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:959)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\tSuppressed: java.io.FileNotFoundException: No such file or directory 's3://geih-datalake-2024/trusted/geih2024_full/part-00003-fbb64ed6-4112-443b-a948-e5c27c681db7-c000.snappy.parquet'\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:535)\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:623)\n",
      "\t\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:523)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:90)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:70)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:69)\n",
      "\t\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t\t... 3 more\n",
      "Caused by: java.lang.RuntimeException: Retry's backoff was interrupted by other process\n",
      "\tat com.amazon.ws.emr.hadoop.fs.util.EmrFsUtils.sleep(EmrFsUtils.java:245)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.S3FSInputStream.read(S3FSInputStream.java:292)\n",
      "\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:284)\n",
      "\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n",
      "\tat java.io.DataInputStream.readFully(DataInputStream.java:195)\n",
      "\tat org.apache.parquet.hadoop.util.H1SeekableInputStream.readFully(H1SeekableInputStream.java:59)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:579)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:845)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:708)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:523)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:69)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "Caused by: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.lang.Thread.sleep(Native Method)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.util.EmrFsUtils.sleep(EmrFsUtils.java:243)\n",
      "\t... 18 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2974)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2910)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2909)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2909)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1263)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1263)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1263)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3173)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3112)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3101)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:154)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:265)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:264)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:541)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:503)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3503)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3502)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4234)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:570)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4232)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:250)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:250)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$8(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:271)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:159)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:69)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4232)\n",
      "\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3502)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://geih-datalake-2024/trusted/abril/part-00000-b6d0950b-7bd3-4756-91a7-9736e3b02936-c000.snappy.parquet, range: 0-5953956, partition values: [empty row], isDataPresent: false, eTag: d038e68fdfc6f3ea49f6e394aa9659a6-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:139)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:447)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:359)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:239)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:731)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:959)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\tSuppressed: java.io.FileNotFoundException: No such file or directory 's3://geih-datalake-2024/trusted/geih2024_full/part-00003-fbb64ed6-4112-443b-a948-e5c27c681db7-c000.snappy.parquet'\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:535)\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:623)\n",
      "\t\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:523)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:90)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:70)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:69)\n",
      "\t\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t\t... 3 more\n",
      "Caused by: java.lang.RuntimeException: Retry's backoff was interrupted by other process\n",
      "\tat com.amazon.ws.emr.hadoop.fs.util.EmrFsUtils.sleep(EmrFsUtils.java:245)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.S3FSInputStream.read(S3FSInputStream.java:292)\n",
      "\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:284)\n",
      "\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n",
      "\tat java.io.DataInputStream.readFully(DataInputStream.java:195)\n",
      "\tat org.apache.parquet.hadoop.util.H1SeekableInputStream.readFully(H1SeekableInputStream.java:59)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:579)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:845)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:708)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:523)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:69)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "Caused by: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.lang.Thread.sleep(Native Method)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.util.EmrFsUtils.sleep(EmrFsUtils.java:243)\n",
      "\t... 18 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1749523038563_0001/container_1749523038563_0001_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 1194, in count\n",
      "    return int(self._jdf.count())\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1749523038563_0001/container_1749523038563_0001_01_000001/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1323, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1749523038563_0001/container_1749523038563_0001_01_000001/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 169, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt1/yarn/usercache/livy/appcache/application_1749523038563_0001/container_1749523038563_0001_01_000001/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\n",
      "    format(target_id, \".\", name), value)\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o28377.count.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 204.0 failed 4 times, most recent failure: Lost task 0.3 in stage 204.0 (TID 359) (ip-172-31-20-52.ec2.internal executor 1): org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://geih-datalake-2024/trusted/abril/part-00000-b6d0950b-7bd3-4756-91a7-9736e3b02936-c000.snappy.parquet, range: 0-5953956, partition values: [empty row], isDataPresent: false, eTag: d038e68fdfc6f3ea49f6e394aa9659a6-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:139)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:447)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:359)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:239)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:731)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:959)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\tSuppressed: java.io.FileNotFoundException: No such file or directory 's3://geih-datalake-2024/trusted/geih2024_full/part-00003-fbb64ed6-4112-443b-a948-e5c27c681db7-c000.snappy.parquet'\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:535)\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:623)\n",
      "\t\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:523)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:90)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:70)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:69)\n",
      "\t\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t\t... 3 more\n",
      "Caused by: java.lang.RuntimeException: Retry's backoff was interrupted by other process\n",
      "\tat com.amazon.ws.emr.hadoop.fs.util.EmrFsUtils.sleep(EmrFsUtils.java:245)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.S3FSInputStream.read(S3FSInputStream.java:292)\n",
      "\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:284)\n",
      "\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n",
      "\tat java.io.DataInputStream.readFully(DataInputStream.java:195)\n",
      "\tat org.apache.parquet.hadoop.util.H1SeekableInputStream.readFully(H1SeekableInputStream.java:59)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:579)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:845)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:708)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:523)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:69)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "Caused by: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.lang.Thread.sleep(Native Method)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.util.EmrFsUtils.sleep(EmrFsUtils.java:243)\n",
      "\t... 18 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2974)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2910)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2909)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2909)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1263)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1263)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1263)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3173)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3112)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3101)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.checkNoFailures(AdaptiveExecutor.scala:154)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.doRun(AdaptiveExecutor.scala:88)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.tryRunningAndGetFuture(AdaptiveExecutor.scala:66)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveExecutor.execute(AdaptiveExecutor.scala:57)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$getFinalPhysicalPlan$1(AdaptiveSparkPlanExec.scala:265)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.getFinalPhysicalPlan(AdaptiveSparkPlanExec.scala:264)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:541)\n",
      "\tat org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:503)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$count$1(Dataset.scala:3503)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$count$1$adapted(Dataset.scala:3502)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4234)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:570)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4232)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:250)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.executeQuery$1(SQLExecution.scala:123)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$9(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withTracker(SQLExecution.scala:250)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$8(SQLExecution.scala:160)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:271)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:159)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:69)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4232)\n",
      "\tat org.apache.spark.sql.Dataset.count(Dataset.scala:3502)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.FileDownloadException: Failed to download file path: s3://geih-datalake-2024/trusted/abril/part-00000-b6d0950b-7bd3-4756-91a7-9736e3b02936-c000.snappy.parquet, range: 0-5953956, partition values: [empty row], isDataPresent: false, eTag: d038e68fdfc6f3ea49f6e394aa9659a6-1\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.next(AsyncFileDownloader.scala:139)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.getNextFile(FileScanRDD.scala:447)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:359)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:239)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:731)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:35)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hasNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:959)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:142)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1541)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\tSuppressed: java.io.FileNotFoundException: No such file or directory 's3://geih-datalake-2024/trusted/geih2024_full/part-00003-fbb64ed6-4112-443b-a948-e5c27c681db7-c000.snappy.parquet'\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:535)\n",
      "\t\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.getFileStatus(EmrFileSystem.java:623)\n",
      "\t\tat org.apache.parquet.hadoop.util.HadoopInputFile.fromPath(HadoopInputFile.java:61)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:523)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:90)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:70)\n",
      "\t\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:69)\n",
      "\t\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t\t... 3 more\n",
      "Caused by: java.lang.RuntimeException: Retry's backoff was interrupted by other process\n",
      "\tat com.amazon.ws.emr.hadoop.fs.util.EmrFsUtils.sleep(EmrFsUtils.java:245)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.S3FSInputStream.read(S3FSInputStream.java:292)\n",
      "\tat java.io.BufferedInputStream.read1(BufferedInputStream.java:284)\n",
      "\tat java.io.BufferedInputStream.read(BufferedInputStream.java:345)\n",
      "\tat java.io.DataInputStream.readFully(DataInputStream.java:195)\n",
      "\tat org.apache.parquet.hadoop.util.H1SeekableInputStream.readFully(H1SeekableInputStream.java:59)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:579)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:845)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:708)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:53)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildPrefetcherWithPartitionValues$1(ParquetFileFormat.scala:523)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader.org$apache$spark$sql$execution$datasources$AsyncFileDownloader$$downloadFile(AsyncFileDownloader.scala:90)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:70)\n",
      "\tat org.apache.spark.sql.execution.datasources.AsyncFileDownloader$$anon$1.call(AsyncFileDownloader.scala:69)\n",
      "\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n",
      "\t... 3 more\n",
      "Caused by: java.lang.InterruptedException: sleep interrupted\n",
      "\tat java.lang.Thread.sleep(Native Method)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.util.EmrFsUtils.sleep(EmrFsUtils.java:243)\n",
      "\t... 18 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when, isnan\n",
    "\n",
    "# Conteo de nulos por columna\n",
    "total = df_geih_2024.count()\n",
    "nulos = df_geih_2024.select([\n",
    "    (count(when(col(c).isNull() | isnan(c), c)) / total).alias(c)\n",
    "    for c in df_geih_2024.columns\n",
    "])\n",
    "nulos.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f54b2f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83dfadd88fc34d429894e84cc7dc8f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'nulos' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'nulos' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nulos_row = nulos.collect()[0].asDict()\n",
    "columnas_a_eliminar = [col for col, prop in nulos_row.items() if prop > 0.4]\n",
    "\n",
    "print(\"Columnas a eliminar (más del 40% nulos):\")\n",
    "print(columnas_a_eliminar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "134e4a42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e89895f3ab9e4f24a5da1bf030950dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'columnas_a_eliminar' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'columnas_a_eliminar' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_limpio = df_geih_2024.drop(*columnas_a_eliminar)\n",
    "\n",
    "# Validar resultados\n",
    "print(\"Columnas iniciales:\", len(df_geih_2024.columns))\n",
    "print(\"Columnas eliminadas:\", len(columnas_a_eliminar))\n",
    "print(\"Columnas finales:\", len(df_limpio.columns))\n",
    "df_limpio.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f2f7c7",
   "metadata": {},
   "source": [
    "## Columnas adicionales a eliminar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ac45a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e299f1cfd95412a964be7200e9185d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_limpio' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_limpio' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lista de columnas a eliminar\n",
    "columnas_del = [\n",
    "    \"p6016\", \"p6030s1\", \"p6030s3\", \"p6050\", \"p2059\", \"p2061\",\n",
    "    \"p1906s1\", \"p1906s2\", \"p1906s3\", \"p1906s4\", \"p1906s5\", \"p1906s6\", \"p1906s7\", \"p1906s8\",\n",
    "    \"p3042s1\", \"p3038\", \"p3039\", \"pob_may18\",\n",
    "    \"80_p3076s1\", \"80_p3076s2\", \"80_p3076s3\", \"80_p3077s1\", \"80_p3077s2\", \"80_p3077s3\",\n",
    "    \"80_p3078s1\", \"80_p3078s2\", \"80_p3078s3\", \"80_p3079s1\", \"80_p3079s2\", \"80_p3079s3\",\n",
    "    \"80_p3081s1\", \"80_p3081s2\", \"80_p3081s3\", \"80_p3082s1\", \"80_p3082s2\", \"80_p3082s3\",\n",
    "    \"80_p3089\", \"80_p3091\", \"80_p3093\", \"80_p3094\", \"80_p3095\", \"80_p3096\", \"80_p3098\", \"80_p3099\", \"80_p3101\",\n",
    "    \"90_p3370\", \"90_p3371\", \"90_p3372\",\n",
    "    \"94_p3373\", \"94_p3373s1\", \"94_p3373s2\", \"94_p3373s3\", \"94_p3376\", \n",
    "    \"94_p3382\", \"94_p3382s1\", \"94_p3382s2\", \"94_p3382s3\", \"94_p3383\",\n",
    "    \"94_p3384\", \"94_p3384s1\", \"94_p3384s2\", \"94_p3384s3\", \"94_p3385\",\n",
    "    \"01_p4000\", \"01_p4010\", \"01_p4020\", \"01_p4030s1\", \"01_p4030s1a1\", \"01_p4030s2\", \n",
    "    \"01_p4030s3\", \"01_p4030s4\", \"01_p4030s4a1\", \"01_p4030s5\",\n",
    "    \"01_p70\", \"01_p5000\", \"01_p5010\", \"01_p5020\", \"01_p5040\", \"01_p5050\", \n",
    "    \"01_p5070\", \"01_p5080\", \"01_p5090\", \"01_p5130\", \"01_p5030\"\n",
    "]\n",
    "\n",
    "# Eliminar las columnas del DataFrame\n",
    "df_limpio = df_limpio.drop(*columnas_del)\n",
    "\n",
    "# Confirmar cantidad de columnas después de eliminación\n",
    "print(f\"Columnas después de eliminación: {len(df_limpio.columns)}\")\n",
    "df_limpio.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d326738b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276e70fb095a4d58b9e6a25f741e49f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_limpio' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_limpio' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Listas de columnas redundantes por módulo\n",
    "columnas_redundantes = [\n",
    "    # periodo, mes, per\n",
    "    \"50_periodo\", \"80_periodo\", \"90_periodo\", \"94_periodo\", \"01_periodo\",\n",
    "    \"50_mes\", \"80_mes\", \"90_mes\", \"94_mes\", \"01_mes\",\n",
    "    \"50_per\", \"80_per\", \"90_per\", \"94_per\", \"01_per\",\n",
    "    \n",
    "    # hogar, regis, clase, dpto, fex_c18\n",
    "    \"50_hogar\", \"80_hogar\", \"90_hogar\", \"94_hogar\", \"01_hogar\",\n",
    "    \"50_regis\", \"80_regis\", \"90_regis\", \"94_regis\", \"01_regis\",\n",
    "    \"50_clase\", \"80_clase\", \"90_clase\", \"94_clase\", \"01_clase\",\n",
    "    \"50_dpto\", \"80_dpto\", \"90_dpto\", \"94_dpto\", \"01_dpto\",\n",
    "    \"50_fex_c18\", \"80_fex_c18\", \"90_fex_c18\", \"94_fex_c18\", \"01_fex_c18\",\n",
    "    \n",
    "    # otros duplicados por módulo\n",
    "    \"94_area\", \"01_area\"\n",
    "]\n",
    "\n",
    "# Eliminar columnas redundantes\n",
    "df_depurado = df_limpio.drop(*columnas_redundantes)\n",
    "\n",
    "# Confirmar estructura\n",
    "print(f\"Total de columnas después de consolidar variables duplicadas: {len(df_depurado.columns)}\")\n",
    "df_depurado.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceac90d7",
   "metadata": {},
   "source": [
    "## Imputación de datos y transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92a28a80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a41a89e4c0804219b2d4e6e66d2bf9ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_depurado' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_depurado' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when, isnan, lit\n",
    "\n",
    "# Total de filas del DataFrame\n",
    "total_filas = df_depurado.count()\n",
    "\n",
    "# Crear lista con proporción de nulos por columna\n",
    "nulos_df = df_depurado.select([\n",
    "    (count(when(col(c).isNull() | isnan(c) | (col(c) == ''), c)) / total_filas).alias(c)\n",
    "    for c in df_depurado.columns\n",
    "])\n",
    "\n",
    "# Transponer el DataFrame de nulos para visualizarlo mejor\n",
    "nulos_por_columna = (\n",
    "    nulos_df.selectExpr(\"stack(\" + str(len(nulos_df.columns)) + \",\" +\n",
    "                        \",\".join([f\"'{c}', `{c}`\" for c in nulos_df.columns]) + \") as (columna, proporcion_nulos)\")\n",
    "    .orderBy(col(\"proporcion_nulos\").desc())\n",
    ")\n",
    "\n",
    "# Mostrar resultado\n",
    "nulos_por_columna.show(n=50, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb8c5d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b721a90a48a34cebb11e3aa4326ad663",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_depurado' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_depurado' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, count, isnan, lit, desc\n",
    "\n",
    "# Variables binarias codificadas como 1 (Sí) y 2 (No)\n",
    "variables_binarias = [\n",
    "    \"p6083\", \"p6081\", \"p2057\", \"p6090\", \"p6160\", \"p6170\", \"01_p5222s2\", \"90_p7495\",\n",
    "    \"90_p7505\", \"90_p3367\", \"p3271\"\n",
    "]\n",
    "\n",
    "# Imputar nulos en variables binarias con 2 (No)\n",
    "for col_name in variables_binarias:\n",
    "    df_depurado = df_depurado.withColumn(\n",
    "        col_name,\n",
    "        when(col(col_name).isNull(), lit(\"2\")).otherwise(col(col_name))\n",
    "    )\n",
    "\n",
    "# Variables categóricas para imputación con la moda\n",
    "variables_categoricas = [\"p6070\", \"p6100\", \"p3042\", \"94_p3374\", \"50_p6240\"]\n",
    "\n",
    "# Imputar nulos en variables categóricas con la moda\n",
    "for var in variables_categoricas:\n",
    "    moda = (\n",
    "        df_depurado.groupBy(var)\n",
    "        .agg(count(\"*\").alias(\"frecuencia\"))\n",
    "        .orderBy(desc(\"frecuencia\"))\n",
    "        .filter(col(var).isNotNull())\n",
    "        .limit(1)\n",
    "        .collect()[0][0]\n",
    "    )\n",
    "    df_depurado = df_depurado.withColumn(\n",
    "        var,\n",
    "        when(col(var).isNull(), lit(moda)).otherwise(col(var))\n",
    "    )\n",
    "\n",
    "# Verificar proporción de nulos después de imputación\n",
    "total = df_depurado.count()\n",
    "proporcion_nulos = df_depurado.select([\n",
    "    (count(when(col(c).isNull() | isnan(c), c)) / total).alias(c)\n",
    "    for c in df_depurado.columns\n",
    "])\n",
    "\n",
    "proporcion_nulos.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aae23380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92778eda87d24395af1ba1d92c7b0117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_depurado' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_depurado' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar tipos de datos\n",
    "df_depurado.printSchema()\n",
    "\n",
    "# Ver muestra de los datos\n",
    "df_depurado.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c650a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a0306a78a7489a8b42a65af8842b41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_depurado' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_depurado' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Convertir columnas a tipo Double para facilitar el tratamiento\n",
    "columnas_convertir = [\n",
    "    \"p3271\", \"p6040\", \"p6083\", \"p6081\", \"p2057\", \"p6080\", \"p6070\",\n",
    "    \"p6090\", \"p6100\", \"p6160\", \"p6170\", \"p3042\", \"50_p6240\",\n",
    "    \"90_p7495\", \"90_p7505\", \"90_p3367\", \"94_p3374\", \"01_p5222s2\", \"01_p6008\"\n",
    "]\n",
    "for col_name in columnas_convertir:\n",
    "    df_depurado = df_depurado.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "\n",
    "# Variables binarias (donde 1 = sí, 2 = no, 9 = no informa)\n",
    "binarias = [\"p3271\", \"p6083\", \"p6081\", \"p6080\", \"p6100\", \"p2057\", \"p6160\", \"p6170\", \"01_p5222s2\", \"p6090\", \"90_p3367\"]\n",
    "for col_name in [\"p3271\", \"p6083\", \"p6081\", \"p2057\", \"p6090\", \"01_p5222s2\", \"90_p3367\"]:\n",
    "    df_depurado = df_depurado.withColumn(\n",
    "        f\"{col_name}_bin\",\n",
    "        when(col(col_name) == 1, 1).when(col(col_name) == 2, 0).otherwise(None)\n",
    "    )\n",
    "\n",
    "# Variables categóricas nominales - aplicar one-hot encoding manual\n",
    "categoricas_nominales = {\n",
    "    \"p6080\": [1, 2, 3, 4, 5, 6],\n",
    "    \"p6070\": [1, 2, 3, 4, 5, 6],\n",
    "    \"p6100\": [1, 2, 3, 4],\n",
    "    \"50_p6240\": [1, 2, 3, 4, 5, 6],\n",
    "}\n",
    "\n",
    "for col_name, categorias in categoricas_nominales.items():\n",
    "    for cat in categorias:\n",
    "        df_depurado = df_depurado.withColumn(\n",
    "            f\"{col_name}_{cat}\",\n",
    "            when(col(col_name) == cat, 1).otherwise(0)\n",
    "        )\n",
    "\n",
    "# Variable ordinal: nivel educativo\n",
    "df_depurado = df_depurado.withColumn(\n",
    "    \"p3042_ord\",\n",
    "    when(col(\"p3042\") == 99, None).otherwise(col(\"p3042\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51a48ce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e55b88eadc442f9a42d219cd7b6334c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_depurado' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_depurado' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar tipos de datos\n",
    "df_depurado.printSchema()\n",
    "\n",
    "# Ver muestra de los datos\n",
    "df_depurado.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dfdec7",
   "metadata": {},
   "source": [
    "## Creación de variable objetivo: INFORMAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1669023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c286c0dc96a415b998d477fb398654c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_depurado' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_depurado' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_depurado = df_depurado.withColumn(\n",
    "    \"INFORMAL\",\n",
    "    when(\n",
    "        (col(\"p6090_bin\") == 0) &\n",
    "        (col(\"01_p5222s2_bin\") == 0) &\n",
    "        (col(\"p6040\") > 18) &\n",
    "        (col(\"50_p6240\") == 1),\n",
    "        1\n",
    "    ).otherwise(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7bbcc84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f924b53f58084ead865ac0cbca25c914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_depurado' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_depurado' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_depurado.groupBy(\"INFORMAL\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a4fedf",
   "metadata": {},
   "source": [
    "## Guardar la Base de Datos depurada en refined/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aefcf7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aeecb4956bff4e64bd0de061e06e66af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_depurado' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_depurado' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ruta_depurado = \"s3://geih-datalake-2024/refined/geih2024_depurada/\"\n",
    "\n",
    "df_depurado.write.mode(\"overwrite\").parquet(ruta_depurado)\n",
    "print(\"La GEIH depurada fue guardada exitosamente en:\", ruta_depurado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a0f98e",
   "metadata": {},
   "source": [
    "## Selección de variables y análisis de correlación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95fcde98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c06deedcfcc4876b62cdcb4995acf1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_depurado' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_depurado' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Variables numéricas continuas\n",
    "variables_continuas = [\"p6040\", \"p3042_ord\", \"01_p6008\"]\n",
    "\n",
    "\n",
    "df_corr = df_depurado\n",
    "for var in variables_continuas:\n",
    "    df_corr = df_corr.withColumn(var, col(var).cast(DoubleType()))\n",
    "    df_corr = df_corr.na.fill({var: 0.0})\n",
    "\n",
    "# VectorAssembler para correlación\n",
    "assembler_corr = VectorAssembler(inputCols=variables_continuas, outputCol=\"features_corr\")\n",
    "df_corr = assembler_corr.transform(df_corr).select(\"features_corr\")\n",
    "\n",
    "# Matriz de correlación\n",
    "correlation_matrix = Correlation.corr(df_corr, \"features_corr\", method=\"pearson\").head()[0].toArray()\n",
    "print(\"Matriz de correlación (Pearson):\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d1c2b",
   "metadata": {},
   "source": [
    "## Reducción de dimensionalidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e45bb503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b388b6cc1f34931aea21143fd981e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_depurado' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_depurado' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler, PCA\n",
    "\n",
    "columnas_modelo = [\n",
    "    \"p3271\", \"p6040\", \"p6083\", \"p6081\", \"p2057\", \"p6080\", \"p6070\", \"p6090\", \"p6100\",\n",
    "    \"p6160\", \"p6170\", \"p3042\", \"50_p6240\", \"90_p7495\", \"90_p7505\", \"90_p3367\",\n",
    "    \"94_p3374\", \"01_p5222s2\", \"01_p6008\", \"p3271_bin\", \"p6083_bin\", \"p6081_bin\",\n",
    "    \"p2057_bin\", \"p6090_bin\", \"01_p5222s2_bin\", \"90_p3367_bin\",\n",
    "    \"p6080_1\", \"p6080_2\", \"p6080_3\", \"p6080_4\", \"p6080_5\", \"p6080_6\",\n",
    "    \"p6070_1\", \"p6070_2\", \"p6070_3\", \"p6070_4\", \"p6070_5\", \"p6070_6\",\n",
    "    \"p6100_1\", \"p6100_2\", \"p6100_3\", \"p6100_4\",\n",
    "    \"50_p6240_1\", \"50_p6240_2\", \"50_p6240_3\", \"50_p6240_4\", \"50_p6240_5\", \"50_p6240_6\",\n",
    "    \"p3042_ord\"\n",
    "]\n",
    "\n",
    "# Asegurar tipo Double y nulos imputados\n",
    "for col_name in columnas_modelo:\n",
    "    df_depurado = df_depurado.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "    df_depurado = df_depurado.na.fill({col_name: 0.0})\n",
    "\n",
    "# VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=columnas_modelo, outputCol=\"features\")\n",
    "df_vect = assembler.transform(df_depurado)\n",
    "\n",
    "# Escalamiento\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withMean=True, withStd=True)\n",
    "scaler_model = scaler.fit(df_vect)\n",
    "df_scaled = scaler_model.transform(df_vect)\n",
    "\n",
    "# PCA\n",
    "pca = PCA(k=5, inputCol=\"scaled_features\", outputCol=\"features_pca\")\n",
    "pca_model = pca.fit(df_scaled)\n",
    "df_pca = pca_model.transform(df_scaled)\n",
    "\n",
    "print(\"Varianza explicada por PCA:\", pca_model.explainedVariance.toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf471d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56359311a0341bda91e7f4ca7b542f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_pca' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_pca' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ruta de guardado en S3\n",
    "ruta_pca = \"s3://geih-datalake-2024/refined/pca/\"\n",
    "\n",
    "# Guardar el DataFrame\n",
    "df_pca.write.mode(\"overwrite\").parquet(ruta_pca)\n",
    "\n",
    "print(\"Datos con PCA guardados exitosamente en:\", ruta_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7e9833",
   "metadata": {},
   "source": [
    "## Modelado Regresión Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "50cd586f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9448c55dc5e440ee993ee65499cb399a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_scaled' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_scaled' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "df_train, df_test = df_scaled.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"INFORMAL\", maxIter=10)\n",
    "modelo_lr = lr.fit(df_train)\n",
    "predicciones_lr = modelo_lr.transform(df_test)\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"INFORMAL\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc_lr = evaluator.evaluate(predicciones_lr)\n",
    "print(\"AUC - Regresión logística:\", auc_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098d9e85",
   "metadata": {},
   "source": [
    "## Undersampling y rebalanceo del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63f53f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c5a815c53941c4800dfa98686e40e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_scaled' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_scaled' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_minoria = df_scaled.filter(col(\"INFORMAL\") == 1)\n",
    "df_mayoria = df_scaled.filter(col(\"INFORMAL\") == 0)\n",
    "\n",
    "# Conteo clase minoritaria\n",
    "count_minoria = df_minoria.count()\n",
    "\n",
    "# Undersample clase mayoritaria al tamaño de la clase minoritaria\n",
    "df_mayoria_sampled = df_mayoria.sample(fraction=count_minoria / df_mayoria.count(), seed=42)\n",
    "\n",
    "# Unir ambos\n",
    "df_balanceado = df_minoria.union(df_mayoria_sampled)\n",
    "\n",
    "# Repartir en entrenamiento y prueba\n",
    "train_data, test_data = df_balanceado.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4dfee727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "204d368e880141e795e667ae4da9bb18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'train_data' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'train_data' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"INFORMAL\", maxIter=10)\n",
    "modelo_lr = lr.fit(train_data)\n",
    "predicciones_lr = modelo_lr.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "32067c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13b1531d6004dde864d91118e6741af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'predicciones_lr' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'predicciones_lr' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "# AUC\n",
    "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"INFORMAL\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc = evaluator_auc.evaluate(predicciones_lr)\n",
    "\n",
    "# F1-score\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"INFORMAL\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "f1 = evaluator_f1.evaluate(predicciones_lr)\n",
    "\n",
    "# Precisión\n",
    "precision = MulticlassClassificationEvaluator(labelCol=\"INFORMAL\", predictionCol=\"prediction\", metricName=\"precisionByLabel\").evaluate(predicciones_lr)\n",
    "\n",
    "# Recall\n",
    "recall = MulticlassClassificationEvaluator(labelCol=\"INFORMAL\", predictionCol=\"prediction\", metricName=\"recallByLabel\").evaluate(predicciones_lr)\n",
    "\n",
    "print(f\"AUC: {auc}\")\n",
    "print(f\"F1-score: {f1}\")\n",
    "print(f\"Precisión (clase 1): {precision}\")\n",
    "print(f\"Recall (clase 1): {recall}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9ea7efe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72fdf85090f34041be8493aa5ea1a1ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'predicciones_lr' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'predicciones_lr' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Matriz de confusión\n",
    "predicciones_lr.groupBy(\"INFORMAL\", \"prediction\").count().orderBy(\"INFORMAL\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800680cc",
   "metadata": {},
   "source": [
    "## Comparación con Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8c129a46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de58a585ccd4f548c2bed4e486e7250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_balanceado' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_balanceado' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Entrenamiento del modelo Random Forest\n",
    "rf = RandomForestClassifier(featuresCol=\"scaled_features\", labelCol=\"INFORMAL\", numTrees=100, maxDepth=5)\n",
    "modelo_rf = rf.fit(df_balanceado)\n",
    "predicciones_rf = modelo_rf.transform(df_balanceado)\n",
    "\n",
    "# Evaluador AUC\n",
    "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"INFORMAL\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "auc_rf = evaluator_auc.evaluate(predicciones_rf)\n",
    "print(\"AUC - Random Forest:\", auc_rf)\n",
    "\n",
    "# Evaluadores adicionales\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"INFORMAL\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "evaluator_precision = MulticlassClassificationEvaluator(labelCol=\"INFORMAL\", predictionCol=\"prediction\", metricName=\"precisionByLabel\")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(labelCol=\"INFORMAL\", predictionCol=\"prediction\", metricName=\"recallByLabel\")\n",
    "\n",
    "f1_rf = evaluator_f1.evaluate(predicciones_rf)\n",
    "precision_rf = evaluator_precision.evaluate(predicciones_rf)\n",
    "recall_rf = evaluator_recall.evaluate(predicciones_rf)\n",
    "\n",
    "print(\"F1-score:\", f1_rf)\n",
    "print(\"Precisión (clase 1):\", precision_rf)\n",
    "print(\"Recall (clase 1):\", recall_rf)\n",
    "\n",
    "# Matriz de confusión\n",
    "matriz_confusion_rf = predicciones_rf.groupBy(\"INFORMAL\", \"prediction\").count()\n",
    "matriz_confusion_rf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0943585c",
   "metadata": {},
   "source": [
    "## Clustering con KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "636e32ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "935588dfd8964407a9701d8a4d9736cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_pca' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_pca' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "scores = []\n",
    "\n",
    "for k in range(2, 7):  # Puedes ajustar este rango\n",
    "    kmeans = KMeans(featuresCol=\"features_pca\", predictionCol=\"cluster\", k=k, seed=1)\n",
    "    model = kmeans.fit(df_pca)\n",
    "    predictions = model.transform(df_pca)\n",
    "\n",
    "    evaluator = ClusteringEvaluator(featuresCol=\"features_pca\", predictionCol=\"cluster\", metricName=\"silhouette\")\n",
    "    score = evaluator.evaluate(predictions)\n",
    "    scores.append((k, score))\n",
    "    print(f\"k={k}, Silhouette Score={score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8635e438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9032fdb65cc34c7c876c5a9eff3abf95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_pca' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_pca' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "# Entrenamiento del modelo KMeans\n",
    "kmeans = KMeans(featuresCol=\"features_pca\", predictionCol=\"cluster\", k=2, seed=1)\n",
    "modelo_kmeans = kmeans.fit(df_pca)\n",
    "\n",
    "# Predicción de clusters\n",
    "df_clusters = modelo_kmeans.transform(df_pca)\n",
    "\n",
    "# Evaluar clustering con Silhouette score\n",
    "evaluator = ClusteringEvaluator(featuresCol=\"features_pca\", predictionCol=\"cluster\", metricName=\"silhouette\", distanceMeasure=\"squaredEuclidean\")\n",
    "silhouette = evaluator.evaluate(df_clusters)\n",
    "print(\"Silhouette Score:\", silhouette)\n",
    "\n",
    "# Ver los centroides\n",
    "centros = modelo_kmeans.clusterCenters()\n",
    "print(\"Centroides del modelo:\")\n",
    "for idx, centroide in enumerate(centros):\n",
    "    print(f\"Cluster {idx}: {centroide}\")\n",
    "\n",
    "# 5. Exportar resultado con asignación de cluster a S3 (ajusta ruta si es necesario)\n",
    "df_clusters.select(\"directorio\", \"secuencia_p\", \"orden\", \"cluster\") \\\n",
    "    .write.mode(\"overwrite\").option(\"header\", \"true\") \\\n",
    "    .csv(\"s3://geih-datalake-2024/refined/output/kmeans_clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c81cae15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aafbb4d58ce9491280610a3699313379",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'df_pca' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'df_pca' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "kmeans = KMeans(featuresCol=\"features_pca\", predictionCol=\"cluster\", k=2, seed=1)\n",
    "modelo_kmeans = kmeans.fit(df_pca)\n",
    "df_clusters = modelo_kmeans.transform(df_pca)\n",
    "\n",
    "silhouette = ClusteringEvaluator(featuresCol=\"features_pca\", predictionCol=\"cluster\", metricName=\"silhouette\").evaluate(df_clusters)\n",
    "print(\"Silhouette Score:\", silhouette)\n",
    "df_clusters.groupBy(\"cluster\", \"INFORMAL\").count().orderBy(\"cluster\", \"INFORMAL\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d373d6",
   "metadata": {},
   "source": [
    "## Guardar resultados en S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83b65138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f5399b312ea40cba1e36dfbccd0d303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "name 'predicciones_lr' is not defined\n",
      "Traceback (most recent call last):\n",
      "NameError: name 'predicciones_lr' is not defined\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ruta_lr = \"s3://geih-datalake-2024/refined/geih2024_depurada/modelos/predicciones_lr/\"\n",
    "predicciones_lr.write.mode(\"overwrite\").csv(ruta_lr, header=True)\n",
    "print(\"Predicciones LR guardadas en:\", ruta_lr)\n",
    "\n",
    "ruta_rf = \"s3://geih-datalake-2024/refined/geih2024_depurada/modelos/predicciones_rf/\"\n",
    "predicciones_rf.write.mode(\"overwrite\").csv(ruta_rf, header=True)\n",
    "print(\"Predicciones RF guardadas en:\", ruta_rf)\n",
    "\n",
    "ruta_cluster = \"s3://geih-datalake-2024/refined/geih2024_depurada/modelos/clustering/\"\n",
    "df_cluster.write.mode(\"overwrite\").csv(ruta_cluster, header=True)\n",
    "print(\"Resultados clustering guardados en:\", ruta_cluster)\n",
    "\n",
    "ruta_confusion = \"s3://geih-datalake-2024/refined/geih2024_depurada/modelos/matriz_confusion_rf/\"\n",
    "matriz_confusion_rf.write.mode(\"overwrite\").csv(ruta_confusion, header=True)\n",
    "print(\"Matriz de confusión RF guardada en:\", ruta_confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b1b274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
